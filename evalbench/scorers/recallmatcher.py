"""Recall matcher."""

# pylint: disable=g-bare-generic]

import json
from typing import Tuple

from scorers.comparator import Comparator
from scorers.comparator import convert_to_set


class RecallMatcher(Comparator):
    """RecallMatcher.

    Attributes:
      name:
    """

    def __init__(self, config: dict):
        """Constructor.

        Args:
            config: Configuration dictionary.
        """
        self.name = "recall_match"
        self.config = config

    def compute_precision_recall(self, golden_results, generated_results):
        """Calculates precision and recall for two sets of results, removing duplicates.

        Args:
            golden_results: A list of the correct results.
            generated_results: A list of the results generated by a model.

        Returns:
            A tuple containing the precision and recall values.
        """

        # Filter out None values (assuming they shouldn't be considered)
        golden_results = (
            [x for x in golden_results if x is not None] if golden_results else []
        )
        generated_results = (
            [x for x in generated_results if x is not None] if generated_results else []
        )

        orig_golden_size = len(golden_results)
        orig_generated_size = len(generated_results)

        golden_results = convert_to_set(golden_results)
        generated_results = convert_to_set(generated_results)

        dedup_golden_size = len(golden_results)
        dedup_generated_size = len(generated_results)

        # Set intersection to find the number of correct predictions
        correct_predictions = len(set(golden_results) & set(generated_results))

        if golden_results == generated_results:
            precision = 1
            recall = 1
        else:
            # Calculate precision and recall
            precision = (
                correct_predictions / len(generated_results) if generated_results else 0
            )
            recall = correct_predictions / len(golden_results) if golden_results else 0

        full_result = {
            "precision": precision,
            "recall": recall,
            "orig_golden_size": orig_golden_size,
            "orig_generated_size": orig_generated_size,
            "dedup_golden_size": dedup_golden_size,
            "dedup_generated_size": dedup_generated_size,
        }
        return full_result

    def compare(
        self,
        nl_prompt: str,
        golden_query: str,
        query_type: str,
        golden_execution_result: list[str],
        golden_eval_result: str,
        golden_error: str,
        generated_query: str,
        generated_execution_result: list[str],
        generated_eval_result: str,
        generated_error: str,
    ) -> Tuple[float, str]:
        """Use the recall strategy to compare results.

        Args:
          golden_query: The golden query.
          golden_execution_result: The golden execution result.
          generated_query: The generated query.
          generated_execution_result: The generated execution result.

        Returns:
          A tuple containing the recall value and a json string of the full result.
        """

        full_result = self.compute_precision_recall(
            golden_execution_result, generated_execution_result
        )
        return full_result["recall"], str(full_result)
